{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmcv.runner import init_dist, load_checkpoint\n",
    "\n",
    "from mmcls.datasets import build_dataloader, build_dataset\n",
    "from mmcls.models import build_classifier\n",
    "from mmcv import tensor2imgs\n",
    "from mmcv.cnn.bricks import ContextBlock\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'configs/imagenet/resnet50_gc-r4_batch256.py'\n",
    "checkpoint_file = 'resnet50_gc-r4.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_outputs = {}\n",
    "\n",
    "\n",
    "def activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        # [N, C, 1, 1]\n",
    "        x = input[0]\n",
    "        context = module.spatial_pool(x)\n",
    "\n",
    "        assert module.channel_add_conv is not None\n",
    "        # [N, C, 1, 1]\n",
    "        channel_add_term = module.channel_add_conv(context)\n",
    "        hidden_outputs[name] = channel_add_term.squeeze(-1).squeeze(-1)\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def register_activation_hook(model):\n",
    "    for module_name, module in model.module.named_modules():\n",
    "        if isinstance(module, ContextBlock):\n",
    "            module.register_forward_hook(activation_hook(module_name))\n",
    "            print(f'{module_name} is registered')\n",
    "\n",
    "\n",
    "\n",
    "activations = dict()\n",
    "\n",
    "\n",
    "def single_gpu_vis(model,\n",
    "                   data_loader,\n",
    "                   show=False,\n",
    "                   out_dir=None):\n",
    "    model.eval()\n",
    "    register_activation_hook(model)\n",
    "\n",
    "    dataset = data_loader.dataset\n",
    "    dataset_length = len(dataset)\n",
    "    prog_bar = mmcv.ProgressBar(len(dataset))\n",
    "    for i, data in enumerate(data_loader):\n",
    "        batch_size = data['img'].size(0)\n",
    "        with torch.no_grad():\n",
    "            model(return_loss=True, **data)\n",
    "\n",
    "        gt_label = data['gt_label'].cuda()\n",
    "\n",
    "        for name in hidden_outputs:\n",
    "            hidden_output = hidden_outputs[name].view(batch_size, -1)\n",
    "            if name not in activations:\n",
    "                activations[name] = hidden_output.new_zeros(\n",
    "                    1000, hidden_output.shape[-1])/data_length\n",
    "            activations[name].scatter_add_(0, gt_label.unsqueeze(\n",
    "                1).expand_as(hidden_output), hidden_output)\n",
    "            activations[name] = activations[name]\n",
    "\n",
    "        hidden_outputs.clear()\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            prog_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile(config_file)\n",
    "# set cudnn_benchmark\n",
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "cfg.model.pretrained = None\n",
    "cfg.data.test.test_mode = True\n",
    "\n",
    "# init distributed env first, since logger depends on the dist info.\n",
    "distributed = False\n",
    "\n",
    "# build the dataloader\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=cfg.data.samples_per_gpu,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=distributed,\n",
    "    shuffle=False,\n",
    "    round_up=False)\n",
    "\n",
    "\n",
    "# build the model and load checkpoint\n",
    "model = build_classifier(cfg.model)\n",
    "checkpoint = load_checkpoint(model, checkpoint_file, map_location='cpu')\n",
    "# old versions did not save class info in checkpoints, this walkaround is\n",
    "# for backward compatibility\n",
    "if 'CLASSES' in checkpoint['meta']:\n",
    "    model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "else:\n",
    "    model.CLASSES = dataset.CLASSES\n",
    "\n",
    "assert not distributed\n",
    "model = MMDataParallel(model, device_ids=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_gpu_vis(model, data_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activations = mmcv.load(osp.join('output/r50_gc-r4_c3', 'activations.pkl'))\n",
    "mapping = mmcv.load('imagenet_class_index.json')\n",
    "vis_indices = [1, 254, 726, 972]\n",
    "step = 8\n",
    "num_samples = 32\n",
    "for name in activations:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    labels = []\n",
    "    for vis_index in vis_indices:\n",
    "        class_name = mapping[str(vis_index)][1]\n",
    "        \n",
    "        step = activations[name].shape[1] // num_samples\n",
    "        x = np.arange(activations[name].shape[1])[::step]\n",
    "        y = activations[name][vis_index][::step]\n",
    "        plt.plot(x, y)\n",
    "#         plt.plot(x, np.exp(y)/sum(np.exp(y)))\n",
    "#         plt.plot(x, np.exp(y)/(np.exp(y)+1))\n",
    "        labels.append(class_name)\n",
    "    class_name = 'all'\n",
    "    x = np.arange(activations[name].shape[1])[::step]\n",
    "    y = activations[name].mean(0)[::step]\n",
    "    axes = plt.gca()\n",
    "#     axes.set_ylim([0, 1])\n",
    "#     plt.plot(x, np.exp(y)/(np.exp(y)+1))\n",
    "#     plt.plot(x, np.exp(y)/sum(np.exp(y)))\n",
    "    plt.plot(x, y)\n",
    "    labels.append(class_name)\n",
    "\n",
    "    plt.legend(labels, ncol=1, loc='upper right',\n",
    "               columnspacing=2.0, labelspacing=1,\n",
    "               handletextpad=0.5, handlelength=1.5,\n",
    "           fancybox=True, shadow=True, fontsize=15)\n",
    "#     plt.title(name)\n",
    "    name_splits = name.split('.')\n",
    "    plt.title('c'+str(int(name_splits[1][-1])+1)+'.'+'.'.join(name_splits[2:]), fontsize=25)\n",
    "    plt.ylabel('Context Amplitude', fontsize=20, labelpad=15)\n",
    "    plt.xlabel('Channel Index', fontsize=20, labelpad=15)\n",
    "    mmcv.mkdir_or_exist('output/r50_gc-r4_c3/vis_act/')\n",
    "#     plt.show()\n",
    "    plt.savefig(f'output/r50_gc-r4_c3/vis_act/{name}.png', bbox_inches='tight')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-mmlab",
   "language": "python",
   "name": "open-mmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
